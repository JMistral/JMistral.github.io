---
title: "Homwwork6 STA578"
author: "Jiaming Chen"
date: "11/1/2017"
output: pdf_document
---

```{r setup, include=FALSE}
library(dplyr)    # data frame manipulations
library(ggplot2)  # plotting

library(caret)
library(glmnet)
```

## 1.a
iii: A model with higher degree of freedom has lower bias term but higher variance term. Lasso regression reduce the degree of freedom by adding a penalty term to the least square error regression. Thus a less flexible model (Lasso) is preferred if its decrease in variance is faster than its increase in in bias.

## 1.b
iii: Same as Lasso regression, Ridge regression is less flexible than least square error regression because of squared penalty term.

## 2.a

iv: When $s=0$ the linear coefficient $\boldsymbol{\beta}=0$, the model is fitting a constant ($df=0$) and the bias is the largest. As we increase $s$, we're increasing the degree of freedom of the model as well, the model can fit the training data better. So the training RSS will decrease steadily.

## 2.b

ii: When the flexibility of the model increase as we increase $s$ from zero, the test RSS decrease initially as the bias term decrease and then increase as the variance increase because of overfitting.

## 2.c

iii: Variance steadily increase as the model flexibility increase. When $s$ increases $|\boldsymbol{\beta}|$ increase as well, the model become more flexible.

## 2.d

iv: Bias steadily decrease because when the model has a higher degree of freedom it can fit the training data better.

## 2.e

v: The irreducible error remains constant because it does not depend the model.

## 3.a

```{r}
data('College', package='ISLR')
set.seed(2112)  
train = College %>% sample_frac(0.5)
test  = setdiff(College, train)
```

## 3.b
The RMSE of LSE model is 1425.535 for training data and 878.7222 for test set.
```{r}
LSE_model <- train( Apps ~ ., data=train, method='lm') 
LSE_model$results

test$yhat.LSE <- predict(LSE_model, newdata=test)
test %>% mutate(resid.LSE = Apps-yhat.LSE) %>%
  summarise( test.RMSE = sqrt(mean( resid.LSE^2 )))
```


## 3.c

The training RMSE for ridge regression with optimized $\lambda$ is 1379.003 and test set RMSE is 878.9444	

```{r}
# Define how we will do our cross validation to select the tuning parameter
ctrl <- trainControl( method='repeatedcv', repeats=4, number=5,  
                      preProcOptions = c('center','scale'))  # center and scale the covariates first!
num = 20    # We did 20 hold out sets (4x5 fold CV)
# Define the grid of tuning parameters we will consider
grid <- data.frame( 
  alpha  = 0,  # 0 => Ridge Regression
  lambda = exp(seq(-5, 8, length=100)) )  # Figured out this range via trial and error

ridge_model <- train( Apps ~ ., data=train, method='glmnet',
                trControl=ctrl, tuneGrid=grid,
                lambda= grid$lambda )   # Not sure why lambda isn't being passed in...

plot.glmnet(ridge_model$finalModel, xvar='lambda')

# Best tuning value
ridge_model$bestTune
best.index <- which.min( ridge_model$results[, 'RMSE'] )
# test error for best model
# I had problem using  predict(ridge_model$finalModel, newdata=test)
ridge_model.best <- train( Apps ~ ., data=train, method='glmnet',
                trControl=ctrl, tuneGrid=ridge_model$bestTune,
                lambda= ridge_model$bestTune ) 
ridge_model.best$results
# predict test set
test$yhat.ridge <- predict(ridge_model.best, newdata=test)
test %>% mutate(resid.ridge = Apps-yhat.ridge) %>%
  summarise( test.RMSE.ridge = sqrt(mean( resid.ridge^2 )))
```

## 3.d

The training RMSE for ridge regression with OneSE $\lambda$ is 1477.003 and test set RMSE is 880.6369

```{r}

# Best tuning value
ridge_model$bestTune
best.index <- which.min( ridge_model$results[, 'RMSE'] )
# Best tuning value within 1 SE of optimal
# Num is the number of hold out sets considered.  4x5=20
bestTuneOneSE <- ridge_model$results %>%
  mutate( index = 1:n() ) %>%
  filter( RMSE <= min( RMSE + RMSESD/sqrt(num) ) ) %>%
  arrange(desc(lambda)) %>%
  slice(1)

bestTuneOneSE.index <- bestTuneOneSE$index
# test error for model using ONE SE
tuneGrid <- bestTuneOneSE %>% dplyr::select(alpha, lambda) %>% as.data.frame()

ridge_model.oneSE <- train( Apps ~ ., data=train, method='glmnet',
                trControl=ctrl, tuneGrid=tuneGrid,
                lambda= tuneGrid$lambda ) 
ridge_model.oneSE$results
# predict test set
test$yhat.ridge.oneSE <- predict(ridge_model.oneSE, newdata=test)
test %>% mutate(resid.ridge.oneSE = Apps-yhat.ridge.oneSE) %>%
  summarise( test.RMSE.ridge.oneSE = sqrt(mean( resid.ridge.oneSE^2 )))
```


## 3.e

The training RMSE for LASSO regression with optimized $\lambda$ is 1369.329 and test set RMSE is 872.3345	

```{r}
# Define the grid of tuning parameters we will consider
grid <- data.frame( 
  alpha  = 1,  # 1 => LASSO
  lambda = exp(seq(-3, 7, length=100)) )  # Figured out this range via trial and error

lasso_model <- train( Apps ~ ., data=train, method='glmnet',
                trControl=ctrl, tuneGrid=grid,
                lambda= grid$lambda )   # Not sure why lambda isn't being passed in...

plot.glmnet(lasso_model$finalModel, xvar='lambda')

# Best tuning value
lasso_model$bestTune
best.index <- which.min( lasso_model$results[, 'RMSE'] )
# test error for best model
lasso_model.best <- train( Apps ~ ., data=train, method='glmnet',
                trControl=ctrl, tuneGrid=lasso_model$bestTune,
                lambda= lasso_model$bestTune ) 
lasso_model.best$results
# predict test set
test$yhat.lasso <- predict(lasso_model.best, newdata=test)
test %>% mutate(resid.lasso = Apps-yhat.lasso) %>%
  summarise( test.RMSE.lasso = sqrt(mean( resid.lasso^2 )))

coef.glmnet( lasso_model.best$finalModel ) 
```


## 3.f


The training RMSE for LASSO regression with OneSE $\lambda$ is 1430.481	 and test set RMSE is 858.865	

```{r}

# Best tuning value
lasso_model$bestTune
best.index <- which.min( lasso_model$results[, 'RMSE'] )
# Best tuning value within 1 SE of optimal
# Num is the number of hold out sets considered.  4x5=20
bestTuneOneSE <- lasso_model$results %>%
  mutate( index = 1:n() ) %>%
  filter( RMSE <= min( RMSE + RMSESD/sqrt(num) ) ) %>%
  arrange(desc(lambda)) %>%
  slice(1)

bestTuneOneSE.index <- bestTuneOneSE$index
# test error for model using ONE SE
tuneGrid <- bestTuneOneSE %>% dplyr::select(alpha, lambda) %>% as.data.frame()

lasso_model.oneSE <- train( Apps ~ ., data=train, method='glmnet',
                trControl=ctrl, tuneGrid=tuneGrid,
                lambda= tuneGrid$lambda ) 
lasso_model.oneSE$results
# predict test set
test$yhat.lasso.oneSE <- predict(lasso_model.oneSE, newdata=test)
test %>% mutate(resid.lasso.oneSE = Apps-yhat.lasso.oneSE) %>%
  summarise( test.RMSE.lasso.oneSE = sqrt(mean( resid.lasso.oneSE^2 )))

coef.glmnet( lasso_model.oneSE$finalModel ) 
```


## 3.g

Generally, all models trained above can fairly predict accurately the application received. We know that R-squared value stands for variance explained and its value should be between 0 and 1. For all model above, we are getting R-squared value around 0.9, which means 90% percent of variance is explained in training data. But there's not significant difference betweem the models trained above, because their R-squared values are quite close (around 1% difference). 
It's surprising to see that the test error is actually lower than training error. But generally when one model's training error rate is higher, the corresponding test error rate is usually lower compared to other models.For example models using OneSE $\lambda$ usually presents a higher training error rate but a lower test error rate compared to corresponding models using best $\lambda$